{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3a29132",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The purpose of this notebook is to provide an example of data cleaning and preparation. In this example, data will be pulled from the Social Security's data set of popular baby names found [here](https://www.ssa.gov/oact/babynames/limits.html). This data will be directly downloaded from the website and organized into one data frame that will be used for a streamlit app."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1f6820",
   "metadata": {},
   "source": [
    "# Downloading the Data\n",
    "First, we'll pull the data directly from the website found [here](https://www.ssa.gov/oact/babynames/limits.html). At first, I thought this was going to be a simple download using the code below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a381cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 403\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Basic download\n",
    "url = \"https://www.ssa.gov/oact/babynames/names.zip\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Save to file\n",
    "with open(\"downloaded_file.zip\", \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "# Check for errors\n",
    "if response.status_code == 200:\n",
    "    print(\"Download successful\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2253ece",
   "metadata": {},
   "source": [
    "I was wrong. After, many code re-writes it looks like this website detects automated requests and blocks them. I'll try to get around this by using Selenium to simulate being an actual user. NOTE: This should be done with caution. This is a sure way to get kicked off a website if you abuse this power. This happened to me during one of my projects at Booz Allen. I used Selenium to webscrape [matweb](https://matweb.com/) and aggregate material information that I needed for the project. I learned the hard way that your IP address will be black listed if you extract too much data. For this project though, I just want to download one link and that's it. So let's begin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbdcd131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "\n",
    "def download_with_selenium_custom_folder(redownload=False):\n",
    "    if redownload==False:\n",
    "        if os.path.exists(os.path.abspath(\"data/names.zip\")):\n",
    "            print(\"File already exists. Skipping download.\")\n",
    "            return\n",
    "            \n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\")\n",
    "    \n",
    "    # Set download directory\n",
    "    download_dir = os.path.abspath(\"data\")  # Downloads to your project's data folder\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "    \n",
    "    prefs = {\n",
    "        \"download.default_directory\": download_dir,\n",
    "        \"download.prompt_for_download\": False,\n",
    "        \"download.directory_upgrade\": True,\n",
    "        \"safebrowsing.enabled\": True\n",
    "    }\n",
    "    options.add_experimental_option(\"prefs\", prefs)\n",
    "    \n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    \n",
    "    try:\n",
    "        # Visit the main page first\n",
    "        driver.get(\"https://www.ssa.gov/oact/babynames/limits.html\")\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Find and click the download link\n",
    "        links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        for link in links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href and \"names.zip\" in href:\n",
    "                print(f\"Downloading from: {href}\")\n",
    "                driver.get(href)\n",
    "                time.sleep(5)  # Wait for download to complete\n",
    "                print(f\"File downloaded to: {download_dir}\")\n",
    "                break\n",
    "                \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# Run the function\n",
    "download_with_selenium_custom_folder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14626050",
   "metadata": {},
   "source": [
    "Worked like a charm! Now it's time to restructre the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d00ae5",
   "metadata": {},
   "source": [
    "# Restructuring the Data\n",
    "The data is in a zip folder so let's unzip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b64b239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip the file\n",
    "import zipfile\n",
    "zip_file_path = os.path.abspath(\"data/names.zip\")\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6c4649",
   "metadata": {},
   "source": [
    "Looks like the data was all stored in text files and there is a ReadMe that states:\n",
    "\n",
    "> **National Data on the relative frequency of given names in the population of\n",
    "> U.S. births where the individual has a Social Security Number**\n",
    "> (Tabulated based on Social Security records as of March 2, 2025)\n",
    "> \n",
    "> For each year of birth YYYY after 1879, we created a comma-delimited file called yobYYYY.txt. Each\n",
    "> record in the individual annual files has the format \"name,sex,number,\" where name is 2 to 15 characters,\n",
    "> sex is M (male) or F (female) and \"number\" is the number of occurrences of the name. Each file is sorted\n",
    "> first on sex and then on number of occurrences in descending order. When there is a tie on the number of\n",
    "> occurrences, names are listed in alphabetical order. This sorting makes it easy to determine a name's rank.\n",
    "> The first record for each sex has rank 1, the second record for each sex has rank 2, and so forth.\n",
    ">\n",
    "> To safeguard privacy, we restrict our list of names to those with at least 5 occurrences\n",
    "\n",
    "Alright, we'll have to read in all of the txt files and aggregate them into one pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f7bd825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>total_count</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2135234</th>\n",
       "      <td>Liam</td>\n",
       "      <td>M</td>\n",
       "      <td>22164</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2135235</th>\n",
       "      <td>Noah</td>\n",
       "      <td>M</td>\n",
       "      <td>20337</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2135236</th>\n",
       "      <td>Oliver</td>\n",
       "      <td>M</td>\n",
       "      <td>15343</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2135237</th>\n",
       "      <td>Theodore</td>\n",
       "      <td>M</td>\n",
       "      <td>12011</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2135238</th>\n",
       "      <td>James</td>\n",
       "      <td>M</td>\n",
       "      <td>11793</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name sex  total_count  year\n",
       "2135234      Liam   M        22164  2024\n",
       "2135235      Noah   M        20337  2024\n",
       "2135236    Oliver   M        15343  2024\n",
       "2135237  Theodore   M        12011  2024\n",
       "2135238     James   M        11793  2024"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in all of the txt files\n",
    "import pandas as pd\n",
    "\n",
    "# get all of the txt files in the data folder\n",
    "txt_files = [f for f in os.listdir(\"data\") if f.endswith(\".txt\")]\n",
    "\n",
    "# read in all of the txt files\n",
    "df = pd.DataFrame()\n",
    "for file in txt_files:\n",
    "    df_add = pd.read_csv(os.path.join(\"data\", file), header=None)\n",
    "    df_add.columns = [\"name\", \"sex\", \"total_count\"]\n",
    "    df_add[\"year\"] = file.split(\"yob\")[1].split(\".txt\")[0]\n",
    "    df = pd.concat([df, df_add])\n",
    "\n",
    "# convert year to int\n",
    "df[\"year\"] = df[\"year\"].astype(int)\n",
    "\n",
    "# drop duplicates and reset index\n",
    "df = df.drop_duplicates(subset=[\"name\", \"sex\", \"year\"])\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# sort by year, sex, and total_count\n",
    "df = df.sort_values(by=[\"year\", \"sex\", \"total_count\"], ascending=False)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3b612c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2149477 entries, 2135234 to 941\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Dtype \n",
      "---  ------       ----- \n",
      " 0   name         object\n",
      " 1   sex          object\n",
      " 2   total_count  int64 \n",
      " 3   year         int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 82.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc89d7e",
   "metadata": {},
   "source": [
    "Let's also update the data so names are persistent each year even if it's total count is zero for a year. For example, if a unique name like Zyler was use 2 times in 2023 but not once in 2024. There isn't a row stating the total count for Zyler was 0 for 2024. I want to include these rows to help perform trend analysis down the road. The reasoning for this might not be apparent now, but you will see why this is important when we start developing projections for popularities of names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05111a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16899750 entries, 0 to 16899749\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Dtype \n",
      "---  ------       ----- \n",
      " 0   name         object\n",
      " 1   sex          object\n",
      " 2   total_count  int64 \n",
      " 3   year         int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 515.7+ MB\n"
     ]
    }
   ],
   "source": [
    "names_sex = df[['name', 'sex']].drop_duplicates()\n",
    "years = pd.DataFrame({'year': df['year'].drop_duplicates()})\n",
    "\n",
    "# Cross join (cartesian product)\n",
    "all_names = names_sex.assign(key=1).merge(years.assign(key=1), on='key').drop('key', axis=1)\n",
    "all_names['total_count'] = 0\n",
    "\n",
    "# merge the data frames\n",
    "df = pd.concat([df, all_names])\n",
    "\n",
    "# drop duplicates and reset index\n",
    "df = df.drop_duplicates(subset=[\"name\", \"sex\", \"year\"])\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670c7daf",
   "metadata": {},
   "source": [
    "Let's add a new feature that calculates the relative popularity of a name for each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ec59377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>total_count</th>\n",
       "      <th>year</th>\n",
       "      <th>popularity_percent</th>\n",
       "      <th>popularity_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Liam</td>\n",
       "      <td>M</td>\n",
       "      <td>22164</td>\n",
       "      <td>2024</td>\n",
       "      <td>0.012921</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Noah</td>\n",
       "      <td>M</td>\n",
       "      <td>20337</td>\n",
       "      <td>2024</td>\n",
       "      <td>0.011856</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oliver</td>\n",
       "      <td>M</td>\n",
       "      <td>15343</td>\n",
       "      <td>2024</td>\n",
       "      <td>0.008945</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Theodore</td>\n",
       "      <td>M</td>\n",
       "      <td>12011</td>\n",
       "      <td>2024</td>\n",
       "      <td>0.007002</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>James</td>\n",
       "      <td>M</td>\n",
       "      <td>11793</td>\n",
       "      <td>2024</td>\n",
       "      <td>0.006875</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       name sex  total_count  year  popularity_percent  popularity_rank\n",
       "0      Liam   M        22164  2024            0.012921                1\n",
       "1      Noah   M        20337  2024            0.011856                2\n",
       "2    Oliver   M        15343  2024            0.008945                3\n",
       "3  Theodore   M        12011  2024            0.007002                4\n",
       "4     James   M        11793  2024            0.006875                5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the relative popularity of a name for each year\n",
    "df[\"popularity_percent\"] = df.groupby([\"sex\", \"year\"])[\"total_count\"].transform(\"sum\")\n",
    "df[\"popularity_percent\"] = df[\"total_count\"] / df[\"popularity_percent\"]\n",
    "\n",
    "df['popularity_rank'] = df.groupby(['sex', 'year'])['popularity_percent'].rank(method='min', ascending=False).astype(int)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3099a2a3",
   "metadata": {},
   "source": [
    "# Create SQLite Database\n",
    "And now we have one giant data frame. Let's save it to a SQLite database and use it for the Streamlit app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f25150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "db_path = \"data/names.db\"\n",
    "if os.path.exists(db_path)==False:\n",
    "    # create a connection to the database\n",
    "    conn = sqlite3.connect(\"data/names.db\")\n",
    "\n",
    "    # save the data frame to a SQLite database  \n",
    "    df.to_sql(\"names\", conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "    # Create indexes for better performance\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('CREATE INDEX idx_name ON names(name)')\n",
    "    cursor.execute('CREATE INDEX idx_sex ON names(sex)')\n",
    "    cursor.execute('CREATE INDEX idx_year ON names(year)')\n",
    "    conn.commit()\n",
    "\n",
    "    # close the connection\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02370882",
   "metadata": {},
   "source": [
    "# Machine Learning Models for Name Popularity Prediction\n",
    "\n",
    "In this section, we'll create multiple machine learning models to predict the future popularity of baby names. We'll train several different models and compare their performance to determine which one works best for this time series prediction task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a26882b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for machine learning\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75a0826",
   "metadata": {},
   "source": [
    "## Data Preparation for Machine Learning\n",
    "\n",
    "First, we need to prepare our data for time series prediction. We'll create features that capture temporal patterns and trends in name popularity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a1a3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to prepare data for a specific name and sex\n",
    "def prepare_name_data(df, name, sex, lookback_years=5):\n",
    "    \"\"\"\n",
    "    Prepare time series data for a specific name and sex combination.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame with name data\n",
    "    - name: Name to analyze\n",
    "    - sex: Gender ('M' or 'F')\n",
    "    - lookback_years: Number of years to look back for features\n",
    "    \n",
    "    Returns:\n",
    "    - X: Feature matrix\n",
    "    - y: Target variable (popularity_percent)\n",
    "    - years: Corresponding years\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter data for specific name and sex\n",
    "    name_data = df[(df['name'] == name) & (df['sex'] == sex)].copy()\n",
    "    name_data = name_data.sort_values('year').reset_index(drop=True)\n",
    "    \n",
    "    if len(name_data) < lookback_years + 1:\n",
    "        return None, None, None\n",
    "    \n",
    "    # Create features\n",
    "    features = []\n",
    "    targets = []\n",
    "    years = []\n",
    "    \n",
    "    for i in range(lookback_years, len(name_data)):\n",
    "        # Target: popularity_percent for current year\n",
    "        target = name_data.iloc[i]['popularity_percent']\n",
    "        \n",
    "        # Features: historical data\n",
    "        feature_row = []\n",
    "        \n",
    "        # Previous years' popularity_percent\n",
    "        for j in range(lookback_years):\n",
    "            feature_row.append(name_data.iloc[i - lookback_years + j]['popularity_percent'])\n",
    "        \n",
    "        # Previous years' total_count\n",
    "        for j in range(lookback_years):\n",
    "            feature_row.append(name_data.iloc[i - lookback_years + j]['total_count'])\n",
    "        \n",
    "        # Previous years' popularity_rank\n",
    "        for j in range(lookback_years):\n",
    "            feature_row.append(name_data.iloc[i - lookback_years + j]['popularity_rank'])\n",
    "        \n",
    "        # Trend features\n",
    "        recent_popularity = name_data.iloc[i-lookback_years:i]['popularity_percent'].values\n",
    "        recent_counts = name_data.iloc[i-lookback_years:i]['total_count'].values\n",
    "        \n",
    "        # Linear trend in popularity\n",
    "        if len(recent_popularity) > 1:\n",
    "            trend_pop = np.polyfit(range(len(recent_popularity)), recent_popularity, 1)[0]\n",
    "        else:\n",
    "            trend_pop = 0\n",
    "        \n",
    "        # Linear trend in counts\n",
    "        if len(recent_counts) > 1:\n",
    "            trend_count = np.polyfit(range(len(recent_counts)), recent_counts, 1)[0]\n",
    "        else:\n",
    "            trend_count = 0\n",
    "        \n",
    "        # Volatility (standard deviation of recent popularity)\n",
    "        volatility = np.std(recent_popularity)\n",
    "        \n",
    "        # Add trend and volatility features\n",
    "        feature_row.extend([trend_pop, trend_count, volatility])\n",
    "        \n",
    "        # Year as a feature (normalized)\n",
    "        year_normalized = (name_data.iloc[i]['year'] - 1880) / (2024 - 1880)\n",
    "        feature_row.append(year_normalized)\n",
    "        \n",
    "        features.append(feature_row)\n",
    "        targets.append(target)\n",
    "        years.append(name_data.iloc[i]['year'])\n",
    "    \n",
    "    return np.array(features), np.array(targets), years\n",
    "\n",
    "# Test the function with a popular name\n",
    "print(\"Testing data preparation with 'Liam' (Male):\")\n",
    "liam_data = prepare_name_data(df, 'Liam', 'M')\n",
    "if liam_data[0] is not None:\n",
    "    print(f\"Feature matrix shape: {liam_data[0].shape}\")\n",
    "    print(f\"Target array shape: {liam_data[1].shape}\")\n",
    "    print(f\"Number of samples: {len(liam_data[2])}\")\n",
    "else:\n",
    "    print(\"Insufficient data for 'Liam'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fd1ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive dataset for training\n",
    "def create_training_dataset(df, min_samples=10, lookback_years=5):\n",
    "    \"\"\"\n",
    "    Create a comprehensive training dataset from all names with sufficient data.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame with name data\n",
    "    - min_samples: Minimum number of samples required for a name to be included\n",
    "    - lookback_years: Number of years to look back for features\n",
    "    \n",
    "    Returns:\n",
    "    - X: Combined feature matrix\n",
    "    - y: Combined target array\n",
    "    - name_sex_pairs: List of (name, sex) pairs for each sample\n",
    "    \"\"\"\n",
    "    \n",
    "    all_features = []\n",
    "    all_targets = []\n",
    "    name_sex_pairs = []\n",
    "    \n",
    "    # Get unique name-sex combinations\n",
    "    unique_combinations = df[['name', 'sex']].drop_duplicates()\n",
    "    \n",
    "    print(f\"Processing {len(unique_combinations)} name-sex combinations...\")\n",
    "    \n",
    "    for idx, row in unique_combinations.iterrows():\n",
    "        name = row['name']\n",
    "        sex = row['sex']\n",
    "        \n",
    "        # Prepare data for this name-sex combination\n",
    "        X, y, years = prepare_name_data(df, name, sex, lookback_years)\n",
    "        \n",
    "        if X is not None and len(X) >= min_samples:\n",
    "            all_features.append(X)\n",
    "            all_targets.append(y)\n",
    "            # Add name-sex pair for each sample\n",
    "            name_sex_pairs.extend([(name, sex)] * len(X))\n",
    "            \n",
    "        if idx % 1000 == 0:\n",
    "            print(f\"Processed {idx}/{len(unique_combinations)} combinations...\")\n",
    "    \n",
    "    # Combine all data\n",
    "    X_combined = np.vstack(all_features)\n",
    "    y_combined = np.concatenate(all_targets)\n",
    "    \n",
    "    print(f\"Final dataset shape: {X_combined.shape}\")\n",
    "    print(f\"Target array shape: {y_combined.shape}\")\n",
    "    \n",
    "    return X_combined, y_combined, name_sex_pairs\n",
    "\n",
    "# Create the training dataset\n",
    "X, y, name_sex_pairs = create_training_dataset(df, min_samples=5, lookback_years=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e305ce",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation\n",
    "\n",
    "Now we'll train multiple machine learning models and compare their performance using appropriate time series cross-validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321ae10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for training and testing\n",
    "# Use time-based split to avoid data leakage\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n",
    "print(f\"Number of features: {X_train.shape[1]}\")\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'XGBoost': xgb.XGBRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "model_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train model\n",
    "    if name == 'Linear Regression':\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Store results\n",
    "    model_results[name] = {\n",
    "        'model': model,\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"{name} Results:\")\n",
    "    print(f\"  RMSE: {rmse:.6f}\")\n",
    "    print(f\"  MAE: {mae:.6f}\")\n",
    "    print(f\"  R²: {r2:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ce494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LSTM model for time series prediction\n",
    "def create_lstm_model(input_shape):\n",
    "    \"\"\"Create an LSTM model for time series prediction.\"\"\"\n",
    "    model = Sequential([\n",
    "        LSTM(50, return_sequences=True, input_shape=input_shape),\n",
    "        Dropout(0.2),\n",
    "        LSTM(50, return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "        Dense(25),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Prepare data for LSTM (needs 3D input: samples, timesteps, features)\n",
    "def prepare_lstm_data(X, y, lookback_years=5):\n",
    "    \"\"\"Prepare data for LSTM model.\"\"\"\n",
    "    # Reshape X to (samples, timesteps, features)\n",
    "    # We'll use the lookback_years as timesteps\n",
    "    n_samples = X.shape[0]\n",
    "    n_features = X.shape[1] // lookback_years  # Each timestep has this many features\n",
    "    \n",
    "    X_lstm = X.reshape(n_samples, lookback_years, n_features)\n",
    "    return X_lstm, y\n",
    "\n",
    "# Prepare LSTM data\n",
    "X_lstm, y_lstm = prepare_lstm_data(X, y, lookback_years=5)\n",
    "X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = train_test_split(\n",
    "    X_lstm, y_lstm, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale the LSTM data\n",
    "scaler_lstm = StandardScaler()\n",
    "X_train_lstm_scaled = scaler_lstm.fit_transform(X_train_lstm.reshape(-1, X_train_lstm.shape[-1]))\n",
    "X_train_lstm_scaled = X_train_lstm_scaled.reshape(X_train_lstm.shape)\n",
    "\n",
    "X_test_lstm_scaled = scaler_lstm.transform(X_test_lstm.reshape(-1, X_test_lstm.shape[-1]))\n",
    "X_test_lstm_scaled = X_test_lstm_scaled.reshape(X_test_lstm.shape)\n",
    "\n",
    "print(f\"LSTM Training data shape: {X_train_lstm_scaled.shape}\")\n",
    "print(f\"LSTM Test data shape: {X_test_lstm_scaled.shape}\")\n",
    "\n",
    "# Create and train LSTM model\n",
    "lstm_model = create_lstm_model((X_train_lstm_scaled.shape[1], X_train_lstm_scaled.shape[2]))\n",
    "\n",
    "print(\"\\nTraining LSTM model...\")\n",
    "history = lstm_model.fit(\n",
    "    X_train_lstm_scaled, y_train_lstm,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lstm = lstm_model.predict(X_test_lstm_scaled, verbose=0).flatten()\n",
    "\n",
    "# Calculate LSTM metrics\n",
    "mse_lstm = mean_squared_error(y_test_lstm, y_pred_lstm)\n",
    "rmse_lstm = np.sqrt(mse_lstm)\n",
    "mae_lstm = mean_absolute_error(y_test_lstm, y_pred_lstm)\n",
    "r2_lstm = r2_score(y_test_lstm, y_pred_lstm)\n",
    "\n",
    "# Store LSTM results\n",
    "model_results['LSTM'] = {\n",
    "    'model': lstm_model,\n",
    "    'mse': mse_lstm,\n",
    "    'rmse': rmse_lstm,\n",
    "    'mae': mae_lstm,\n",
    "    'r2': r2_lstm,\n",
    "    'predictions': y_pred_lstm\n",
    "}\n",
    "\n",
    "print(f\"LSTM Results:\")\n",
    "print(f\"  RMSE: {rmse_lstm:.6f}\")\n",
    "print(f\"  MAE: {mae_lstm:.6f}\")\n",
    "print(f\"  R²: {r2_lstm:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c85dea7",
   "metadata": {},
   "source": [
    "## Model Performance Comparison\n",
    "\n",
    "Let's compare the performance of all models and create visualizations to understand their strengths and weaknesses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7263a9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance comparison\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_data = []\n",
    "for model_name, results in model_results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'RMSE': results['rmse'],\n",
    "        'MAE': results['mae'],\n",
    "        'R²': results['r2']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('RMSE')\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "print(comparison_df.to_string(index=False, float_format='%.6f'))\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# RMSE comparison\n",
    "axes[0, 0].bar(comparison_df['Model'], comparison_df['RMSE'], color='skyblue')\n",
    "axes[0, 0].set_title('Root Mean Square Error (RMSE)')\n",
    "axes[0, 0].set_ylabel('RMSE')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# MAE comparison\n",
    "axes[0, 1].bar(comparison_df['Model'], comparison_df['MAE'], color='lightcoral')\n",
    "axes[0, 1].set_title('Mean Absolute Error (MAE)')\n",
    "axes[0, 1].set_ylabel('MAE')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# R² comparison\n",
    "axes[1, 0].bar(comparison_df['Model'], comparison_df['R²'], color='lightgreen')\n",
    "axes[1, 0].set_title('R² Score')\n",
    "axes[1, 0].set_ylabel('R²')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Predictions vs Actual (using best model)\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "best_predictions = model_results[best_model_name]['predictions']\n",
    "\n",
    "axes[1, 1].scatter(y_test, best_predictions, alpha=0.5, s=1)\n",
    "axes[1, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[1, 1].set_xlabel('Actual Values')\n",
    "axes[1, 1].set_ylabel('Predicted Values')\n",
    "axes[1, 1].set_title(f'Predictions vs Actual ({best_model_name})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b278f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional analysis: Feature importance for tree-based models\n",
    "def plot_feature_importance(model, model_name, feature_names=None):\n",
    "    \"\"\"Plot feature importance for tree-based models.\"\"\"\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "        \n",
    "        if feature_names is None:\n",
    "            feature_names = [f'Feature_{i}' for i in range(len(importances))]\n",
    "        \n",
    "        # Get top 20 most important features\n",
    "        indices = np.argsort(importances)[::-1][:20]\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.title(f'Feature Importance - {model_name}')\n",
    "        plt.bar(range(len(indices)), importances[indices])\n",
    "        plt.xticks(range(len(indices)), [feature_names[i] for i in indices], rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Create feature names for interpretation\n",
    "feature_names = []\n",
    "lookback_years = 5\n",
    "\n",
    "# Historical popularity_percent features\n",
    "for i in range(lookback_years):\n",
    "    feature_names.append(f'popularity_percent_t-{lookback_years-i-1}')\n",
    "\n",
    "# Historical total_count features\n",
    "for i in range(lookback_years):\n",
    "    feature_names.append(f'total_count_t-{lookback_years-i-1}')\n",
    "\n",
    "# Historical popularity_rank features\n",
    "for i in range(lookback_years):\n",
    "    feature_names.append(f'popularity_rank_t-{lookback_years-i-1}')\n",
    "\n",
    "# Trend and volatility features\n",
    "feature_names.extend(['trend_popularity', 'trend_count', 'volatility', 'year_normalized'])\n",
    "\n",
    "print(f\"Total features: {len(feature_names)}\")\n",
    "print(\"Feature names:\", feature_names[:10], \"...\")\n",
    "\n",
    "# Plot feature importance for Random Forest and XGBoost\n",
    "for model_name in ['Random Forest', 'XGBoost']:\n",
    "    if model_name in model_results:\n",
    "        plot_feature_importance(model_results[model_name]['model'], model_name, feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6640850b",
   "metadata": {},
   "source": [
    "## Model Assessment and Recommendations\n",
    "\n",
    "Based on the performance metrics and analysis, let's provide a comprehensive assessment of which model should be used for predicting name popularity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca0d31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive model assessment\n",
    "def assess_model_performance(model_results):\n",
    "    \"\"\"Provide comprehensive assessment of model performance.\"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"COMPREHENSIVE MODEL ASSESSMENT\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Sort models by RMSE (lower is better)\n",
    "    sorted_models = sorted(model_results.items(), key=lambda x: x[1]['rmse'])\n",
    "    \n",
    "    print(\"\\n1. PERFORMANCE RANKING (by RMSE):\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, (name, results) in enumerate(sorted_models, 1):\n",
    "        print(f\"{i}. {name}:\")\n",
    "        print(f\"   RMSE: {results['rmse']:.6f}\")\n",
    "        print(f\"   MAE:  {results['mae']:.6f}\")\n",
    "        print(f\"   R²:   {results['r2']:.6f}\")\n",
    "        print()\n",
    "    \n",
    "    # Best model\n",
    "    best_model_name, best_results = sorted_models[0]\n",
    "    \n",
    "    print(\"2. BEST PERFORMING MODEL:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Model: {best_model_name}\")\n",
    "    print(f\"RMSE: {best_results['rmse']:.6f}\")\n",
    "    print(f\"MAE: {best_results['mae']:.6f}\")\n",
    "    print(f\"R²: {best_results['r2']:.6f}\")\n",
    "    print()\n",
    "    \n",
    "    # Model characteristics analysis\n",
    "    print(\"3. MODEL CHARACTERISTICS:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    for name, results in model_results.items():\n",
    "        print(f\"\\n{name}:\")\n",
    "        \n",
    "        # Interpretability\n",
    "        if name == 'Linear Regression':\n",
    "            print(\"  • High interpretability - coefficients show feature importance\")\n",
    "            print(\"  • Assumes linear relationships\")\n",
    "        elif name in ['Random Forest', 'XGBoost']:\n",
    "            print(\"  • Medium interpretability - feature importance available\")\n",
    "            print(\"  • Can capture non-linear relationships\")\n",
    "        elif name == 'LSTM':\n",
    "            print(\"  • Low interpretability - black box model\")\n",
    "            print(\"  • Designed for sequential/time series data\")\n",
    "        \n",
    "        # Training time and complexity\n",
    "        if name == 'Linear Regression':\n",
    "            print(\"  • Fast training and prediction\")\n",
    "            print(\"  • Low computational requirements\")\n",
    "        elif name in ['Random Forest', 'XGBoost']:\n",
    "            print(\"  • Moderate training time\")\n",
    "            print(\"  • Good for handling mixed data types\")\n",
    "        elif name == 'LSTM':\n",
    "            print(\"  • Slowest training time\")\n",
    "            print(\"  • Requires more data and computational resources\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(\"\\n4. RECOMMENDATIONS:\")\n",
    "    print(\"-\" * 20)\n",
    "    \n",
    "    print(f\"\\nPRIMARY RECOMMENDATION: {best_model_name}\")\n",
    "    print(f\"Reason: Lowest RMSE ({best_results['rmse']:.6f}) indicates best overall accuracy\")\n",
    "    \n",
    "    # Additional recommendations based on use case\n",
    "    print(\"\\nUSE CASE RECOMMENDATIONS:\")\n",
    "    print(\"• For quick predictions and interpretability: Linear Regression\")\n",
    "    print(\"• For robust predictions with feature insights: Random Forest or XGBoost\")\n",
    "    print(\"• For complex time series patterns: LSTM (if computational resources allow)\")\n",
    "    print(\"• For production deployment: XGBoost (good balance of performance and speed)\")\n",
    "    \n",
    "    # Model limitations\n",
    "    print(\"\\n5. MODEL LIMITATIONS:\")\n",
    "    print(\"-\" * 20)\n",
    "    print(\"• All models assume historical patterns will continue\")\n",
    "    print(\"• External factors (cultural trends, events) not captured\")\n",
    "    print(\"• Predictions become less reliable further into the future\")\n",
    "    print(\"• Models may not handle sudden popularity spikes well\")\n",
    "    \n",
    "    return best_model_name, best_results\n",
    "\n",
    "# Run the assessment\n",
    "best_model, best_results = assess_model_performance(model_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5a375d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model and scaler for future use\n",
    "import joblib\n",
    "\n",
    "# Save the best model\n",
    "best_model_obj = model_results[best_model]['model']\n",
    "joblib.dump(best_model_obj, f'data/best_model_{best_model.lower().replace(\" \", \"_\")}.pkl')\n",
    "\n",
    "# Save the scaler\n",
    "if best_model == 'Linear Regression':\n",
    "    joblib.dump(scaler, 'data/scaler.pkl')\n",
    "elif best_model == 'LSTM':\n",
    "    joblib.dump(scaler_lstm, 'data/scaler_lstm.pkl')\n",
    "\n",
    "# Save feature names for future reference\n",
    "joblib.dump(feature_names, 'data/feature_names.pkl')\n",
    "\n",
    "print(f\"\\n6. MODEL PERSISTENCE:\")\n",
    "print(\"-\" * 20)\n",
    "print(f\"Best model ({best_model}) saved to: data/best_model_{best_model.lower().replace(' ', '_')}.pkl\")\n",
    "print(\"Scaler saved to: data/scaler.pkl\")\n",
    "print(\"Feature names saved to: data/feature_names.pkl\")\n",
    "print(\"\\nThese files can be loaded in your Streamlit app for making predictions!\")\n",
    "\n",
    "# Create a prediction function for future use\n",
    "def predict_name_popularity(name, sex, years_ahead=1, model_name=best_model):\n",
    "    \"\"\"\n",
    "    Predict future popularity for a given name and sex.\n",
    "    \n",
    "    Parameters:\n",
    "    - name: Name to predict\n",
    "    - sex: Gender ('M' or 'F')\n",
    "    - years_ahead: Number of years to predict ahead\n",
    "    - model_name: Model to use for prediction\n",
    "    \n",
    "    Returns:\n",
    "    - predictions: List of predicted popularity_percent values\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get historical data for the name\n",
    "    name_data = df[(df['name'] == name) & (df['sex'] == sex)].copy()\n",
    "    name_data = name_data.sort_values('year').reset_index(drop=True)\n",
    "    \n",
    "    if len(name_data) < 5:\n",
    "        return None, \"Insufficient historical data\"\n",
    "    \n",
    "    # Prepare features for prediction\n",
    "    X_pred, _, _ = prepare_name_data(df, name, sex, lookback_years=5)\n",
    "    \n",
    "    if X_pred is None or len(X_pred) == 0:\n",
    "        return None, \"Could not prepare features\"\n",
    "    \n",
    "    # Use the most recent data point for prediction\n",
    "    latest_features = X_pred[-1:].copy()\n",
    "    \n",
    "    # Load the appropriate model and scaler\n",
    "    model = joblib.load(f'data/best_model_{model_name.lower().replace(\" \", \"_\")}.pkl')\n",
    "    \n",
    "    if model_name == 'Linear Regression':\n",
    "        scaler = joblib.load('data/scaler.pkl')\n",
    "        latest_features = scaler.transform(latest_features)\n",
    "    elif model_name == 'LSTM':\n",
    "        scaler = joblib.load('data/scaler_lstm.pkl')\n",
    "        # Reshape for LSTM\n",
    "        latest_features = latest_features.reshape(1, 5, -1)\n",
    "        latest_features = scaler.transform(latest_features.reshape(-1, latest_features.shape[-1]))\n",
    "        latest_features = latest_features.reshape(1, 5, -1)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(latest_features)\n",
    "    \n",
    "    return prediction[0], \"Success\"\n",
    "\n",
    "# Test the prediction function\n",
    "print(f\"\\n7. PREDICTION FUNCTION TEST:\")\n",
    "print(\"-\" * 30)\n",
    "test_name = \"Liam\"\n",
    "test_sex = \"M\"\n",
    "prediction, status = predict_name_popularity(test_name, test_sex, years_ahead=1)\n",
    "\n",
    "if status == \"Success\":\n",
    "    print(f\"Predicted popularity for {test_name} ({test_sex}) next year: {prediction:.6f}\")\n",
    "    print(f\"Current popularity: {df[(df['name'] == test_name) & (df['sex'] == test_sex) & (df['year'] == 2024)]['popularity_percent'].iloc[0]:.6f}\")\n",
    "else:\n",
    "    print(f\"Prediction failed: {status}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
